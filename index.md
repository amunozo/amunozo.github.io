---
layout: default
title: About me
permalink: /
---

<div style="float: left; margin-right: 20px; margin-bottom: 10px;">
  <img src="./image.png" alt="My Picture" style="border-radius: 50%; width: 175px; height: auto;" />
</div>

Hi! I'm **Alberto Muñoz-Ortiz**, a PhD student in Natural Language Processing at the [LyS group](https://www.grupolys.org/) at [Universidade da Coruña](https://www.udc.es/) in A Coruña, Spain. My doctoral research is supervised by [David Vilares](http://www.grupolys.org/~david.vilares/) and [Carlos Gómez-Rodríguez](http://www.grupolys.org/~cgomezr/). I'm currently focused on writing my thesis dissertation and **actively seeking a job in industry**.

Previously, I was a visiting researcher at the [MaiNLP Research Lab](https://mainlp.github.io/) at [LMU](https://www.lmu.de/en/) in Munich, Germany, hosted by [Barbara Plank](https://bplank.github.io/) during the summer of 2023. Together with Barbara and [Verena Blaschke](https://verenablaschke.github.io/), we explored the use of **pixel-based models to transfer knowledge** from Standard German to German non-standard varieties.

More recently, I visited the [NLP Lab](https://nlp.epfl.ch/) at [EPFL](https://www.epfl.ch/schools/ic/) in Lausanne, Switzerland, from October 2024 to March 2025. There, hosted by [Antoine Bosselut](https://atcbosselut.github.io/) and [Gail Weiss](https://gailweiss.github.io/), we explored the **grokking phenomenon in autoregressive transformers** and the importance of **basic facts in training data for model generalization**.

My research investigates **linguistic structure** from two complementary perspectives. On one hand, I use it to improve the **performance, efficiency, and robustness** of NLP systems, especially in **low-resource or non-standardized language** settings. This involves exploring **novel representations**, like linearizing complex tasks such as Nested NER into simple sequence labeling and using **pixel-based visual models** for **transfer learning**.

Alongside this, I apply these same linguistic structures as an analytical tool to **interpret and understand** language models. This includes probing their internal knowledge using **syntactic dependencies** and analyzing the distinct linguistic patterns found in text generated by large language models, comparing them to human patterns.

Beyond my core NLP research, I'm deeply interested in how **deep learning can address challenges** by augmenting human capabilities where our intuition might fall short, such as science. I believe that AI research should prioritize increasing our collective problem-solving capacity, tackling issues we can't solve alone, rather than merely automating or accelerating existing processes.
