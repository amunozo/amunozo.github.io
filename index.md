---
layout: default
title: About me
permalink: /
---

<div style="float: left; margin-right: 20px; margin-bottom: 10px;">
  <img src="./image.png" alt="My Picture" style="border-radius: 50%; width: 175px; height: auto;" />
</div>

Hi! I'm **Alberto Muñoz-Ortiz**, a PhD student in Natural Language Processing at the [LyS group](https://www.grupolys.org/) at [Universidade da Coruña](https://www.udc.es/) in A Coruña, Spain. My doctoral research is supervised by [David Vilares](http://www.grupolys.org/~david.vilares/) and [Carlos Gómez-Rodríguez](http://www.grupolys.org/~cgomezr/). I'm currently focused on writing my thesis dissertation and **actively seeking a job in industry**.

My core research focuses on innovative **data-efficient approaches** to enhance NLP system performance, particularly in **low-resource environments**. I achieve this by leveraging techniques such as **transfer learning, novel representations, synthetic data generation**, and the integration of linguistic information, particularly **syntactic dependencies**.

Beyond my core NLP research, I'm deeply interested in how **deep learning can address challenges in science and medicine**, specifically by augmenting human capabilities where our intuition might fall short. I believe that AI research should prioritize increasing our collective problem-solving capacity, tackling issues we can't solve alone, rather than merely automating or accelerating existing processes.

Previously, I was a visiting researcher at the [MaiNLP Research Lab](https://mainlp.github.io/) at [LMU](https://www.lmu.de/en/) in Munich, Germany, hosted by [Barbara Plank](https://bplank.github.io/) during the summer of 2023. Together with Barbara and [Verena Blaschke](https://verenablaschke.github.io/), we explored the use of **pixel-based models to transfer knowledge** from Standard German to German non-standard varieties.

More recently, I visited the [NLP Lab](https://nlp.epfl.ch/) at [EPFL](https://www.epfl.ch/schools/ic/) in Lausanne, Switzerland, from October 2024 to March 2025. There, hosted by [Antoine Bosselut](https://atcbosselut.github.io/) and [Gail Weiss](https://gailweiss.github.io/), we explored the **grokking phenomenon in autoregressive transformers** and the importance of **basic facts in training data for model generalization**.
